{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_Korean.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1aFioqtqlalELkGDxV90iV1acl-qxfFGf","authorship_tag":"ABX9TyM08btlvZHZx9o26kVL9HEo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"a7f581c7a6b74bd18903604993fec894":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3d691f77b854536a0dbbef043e41b3a","IPY_MODEL_ea66c4110d1f4ab391d7c6310d306483","IPY_MODEL_57721c9937074e579b99a16ee54513f1"],"layout":"IPY_MODEL_bdb456559b2047d18a47c93348eb39b5"}},"a3d691f77b854536a0dbbef043e41b3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12c42ff4f57240a98c87fac51460b998","placeholder":"​","style":"IPY_MODEL_9b9cc8d6b0d84e95a4e692c60f1b4b24","value":"Downloading: 100%"}},"ea66c4110d1f4ab391d7c6310d306483":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e13499bad524110929063025fe9eca4","max":995526,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f6d4dff430ba4e5dabc458d4576af537","value":995526}},"57721c9937074e579b99a16ee54513f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2688eb1e0f654545b4fa8b55e1096c7a","placeholder":"​","style":"IPY_MODEL_492e7b9eb4bc4698a244c1214bd27d22","value":" 972k/972k [00:01&lt;00:00, 1.15MB/s]"}},"bdb456559b2047d18a47c93348eb39b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12c42ff4f57240a98c87fac51460b998":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b9cc8d6b0d84e95a4e692c60f1b4b24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e13499bad524110929063025fe9eca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6d4dff430ba4e5dabc458d4576af537":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2688eb1e0f654545b4fa8b55e1096c7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"492e7b9eb4bc4698a244c1214bd27d22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fce563cfe9c241c4b95675d2f935b0c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20038924f92743f0a2e560f2059674fa","IPY_MODEL_2ed9aeaaa3c74ce7b44d5ae206956a37","IPY_MODEL_51b8207535d647bb82d77535b62e838b"],"layout":"IPY_MODEL_39ed821e302e4544ba52985c80e428f5"}},"20038924f92743f0a2e560f2059674fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d84d8c4124074a7e9ea45c5f744a5890","placeholder":"​","style":"IPY_MODEL_31868891d6c94d7f9029b42f695bae6c","value":"Downloading: 100%"}},"2ed9aeaaa3c74ce7b44d5ae206956a37":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43f53fa8294648f6906e5b4bd876db77","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11d033e845cd457f928964a74d1a9674","value":29}},"51b8207535d647bb82d77535b62e838b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36873fc85ff74065b76889c4da03a653","placeholder":"​","style":"IPY_MODEL_d52bde1c0e6142ad9b903bf33dce96ec","value":" 29.0/29.0 [00:00&lt;00:00, 418B/s]"}},"39ed821e302e4544ba52985c80e428f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d84d8c4124074a7e9ea45c5f744a5890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31868891d6c94d7f9029b42f695bae6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43f53fa8294648f6906e5b4bd876db77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11d033e845cd457f928964a74d1a9674":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36873fc85ff74065b76889c4da03a653":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d52bde1c0e6142ad9b903bf33dce96ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"653cd24ff9db41b5be4e408788ad7868":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2ab5ec20211487d8b4bb1b5ac4a6688","IPY_MODEL_60b2b5dbb091488c9a5024855698ec49","IPY_MODEL_0fe22278fdb04aa19d78c9a8f8d09ced"],"layout":"IPY_MODEL_9cd83e8e540d4f1ca681abd2806aa489"}},"d2ab5ec20211487d8b4bb1b5ac4a6688":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1ac55bab73a4986b5c6c9df4a2d50da","placeholder":"​","style":"IPY_MODEL_90b77c56f9884cd4a0e2854bc82d197c","value":"Downloading: 100%"}},"60b2b5dbb091488c9a5024855698ec49":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_083c2a3b3371444fb0d3802dfb3cf38b","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8381a63091e49b1a373412179f03554","value":625}},"0fe22278fdb04aa19d78c9a8f8d09ced":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c7adc5a73854393af4b1a267bd6726e","placeholder":"​","style":"IPY_MODEL_71624ba878794718852ab23224e526fb","value":" 625/625 [00:00&lt;00:00, 5.47kB/s]"}},"9cd83e8e540d4f1ca681abd2806aa489":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1ac55bab73a4986b5c6c9df4a2d50da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90b77c56f9884cd4a0e2854bc82d197c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"083c2a3b3371444fb0d3802dfb3cf38b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8381a63091e49b1a373412179f03554":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c7adc5a73854393af4b1a267bd6726e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71624ba878794718852ab23224e526fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e733d90da32d4a0fb7fa2b24bb8aaa6b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f0a89aa20a9423cb785a2b7db0b47b6","IPY_MODEL_91316c8f5bb345b0b51bd507c48e2bfb","IPY_MODEL_da3daac4b75f4a379eaccfaed83a0b36"],"layout":"IPY_MODEL_dee4d6002fc848e783470318351b8cdf"}},"6f0a89aa20a9423cb785a2b7db0b47b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b94577fba0344908827fbca9146406e","placeholder":"​","style":"IPY_MODEL_5856fb66a69a498c984a34e0f1ad02c0","value":"Downloading: 100%"}},"91316c8f5bb345b0b51bd507c48e2bfb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ad47530c879443daeb0d3d46b4093b8","max":714314041,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6caedb01a9f34e8ba9e3469fa943f9a7","value":714314041}},"da3daac4b75f4a379eaccfaed83a0b36":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cad4b0ad9743406a8d80c2bd1b1e23df","placeholder":"​","style":"IPY_MODEL_074744d89f204c9b920c8c690edbdb68","value":" 681M/681M [00:26&lt;00:00, 48.1MB/s]"}},"dee4d6002fc848e783470318351b8cdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b94577fba0344908827fbca9146406e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5856fb66a69a498c984a34e0f1ad02c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ad47530c879443daeb0d3d46b4093b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6caedb01a9f34e8ba9e3469fa943f9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cad4b0ad9743406a8d80c2bd1b1e23df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"074744d89f204c9b920c8c690edbdb68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d77456f60974e1bbbe54953bb4005cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8df437f2793944ae911362ef90f93a88","IPY_MODEL_a9a6ab36d2824c12a003826e96a7ce81","IPY_MODEL_ddaea7dcd8aa4a0bac2442920a3dfb98"],"layout":"IPY_MODEL_dd6d2640f040440191e14a2bbca1b82c"}},"8df437f2793944ae911362ef90f93a88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7747640cf0344e1eb074716ab8e55710","placeholder":"​","style":"IPY_MODEL_f252cfa89bb24ca0a9677d4c58ba50c3","value":"Downloading builder script: "}},"a9a6ab36d2824c12a003826e96a7ce81":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc6226d7b73147c19d47696f64c256c3","max":1652,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce4dc34045be4e03a92397ebec80d207","value":1652}},"ddaea7dcd8aa4a0bac2442920a3dfb98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c16cf030afe84236bb2fb90de9768bce","placeholder":"​","style":"IPY_MODEL_ce9bef725e134692a28d29b2d6e512cc","value":" 4.21k/? [00:00&lt;00:00, 99.9kB/s]"}},"dd6d2640f040440191e14a2bbca1b82c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7747640cf0344e1eb074716ab8e55710":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f252cfa89bb24ca0a9677d4c58ba50c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc6226d7b73147c19d47696f64c256c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce4dc34045be4e03a92397ebec80d207":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c16cf030afe84236bb2fb90de9768bce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce9bef725e134692a28d29b2d6e512cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 다중 언어를 위한(multilingual) BERT"],"metadata":{"id":"ZdfYoZkJLaZX"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets"],"metadata":{"id":"wbmoQ_LxNzW9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**다음 영화 리뷰 데이터 준비**"],"metadata":{"id":"ZR361q_4L_jz"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hj4d7DPnLU3F","executionInfo":{"status":"ok","timestamp":1657164850923,"user_tz":-540,"elapsed":991,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"1005c547-e61e-472c-e539-90f49b30726f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train set size: 8282\n","Validation set size: 2761\n","Test set size: 3682\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","df = pd.read_csv('/content/drive/Othercomputers/내 컴퓨터/TextMining/data/daum_movie_review.csv')\n","\n","# rating이 6보다 작으면 0 즉 부정, 6 이상이면 긍정으로 라벨 생성\n","y = [0 if rate < 6 else 1 for rate in df.rating]\n","\n","# 데이터셋을 학습, 검증, 평가의 세 데이터셋으로 분리\n","X_train_val, X_test, y_train_val, y_test = train_test_split(df.review.tolist(), y, random_state=0)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0)\n","\n","print('Train set size:', len(X_train))\n","print('Validation set size:', len(X_val))\n","print('Test set size:', len(X_test))"]},{"cell_type":"markdown","source":["**토크나이저 및 모델(분류기) 설정**"],"metadata":{"id":"iklWR9MSN4he"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification \n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","# 토크나이징 예시\n","print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n","inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n","print(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":170,"referenced_widgets":["a7f581c7a6b74bd18903604993fec894","a3d691f77b854536a0dbbef043e41b3a","ea66c4110d1f4ab391d7c6310d306483","57721c9937074e579b99a16ee54513f1","bdb456559b2047d18a47c93348eb39b5","12c42ff4f57240a98c87fac51460b998","9b9cc8d6b0d84e95a4e692c60f1b4b24","7e13499bad524110929063025fe9eca4","f6d4dff430ba4e5dabc458d4576af537","2688eb1e0f654545b4fa8b55e1096c7a","492e7b9eb4bc4698a244c1214bd27d22","fce563cfe9c241c4b95675d2f935b0c4","20038924f92743f0a2e560f2059674fa","2ed9aeaaa3c74ce7b44d5ae206956a37","51b8207535d647bb82d77535b62e838b","39ed821e302e4544ba52985c80e428f5","d84d8c4124074a7e9ea45c5f744a5890","31868891d6c94d7f9029b42f695bae6c","43f53fa8294648f6906e5b4bd876db77","11d033e845cd457f928964a74d1a9674","36873fc85ff74065b76889c4da03a653","d52bde1c0e6142ad9b903bf33dce96ec","653cd24ff9db41b5be4e408788ad7868","d2ab5ec20211487d8b4bb1b5ac4a6688","60b2b5dbb091488c9a5024855698ec49","0fe22278fdb04aa19d78c9a8f8d09ced","9cd83e8e540d4f1ca681abd2806aa489","b1ac55bab73a4986b5c6c9df4a2d50da","90b77c56f9884cd4a0e2854bc82d197c","083c2a3b3371444fb0d3802dfb3cf38b","a8381a63091e49b1a373412179f03554","8c7adc5a73854393af4b1a267bd6726e","71624ba878794718852ab23224e526fb"]},"id":"iq5arwDINEDU","executionInfo":{"status":"ok","timestamp":1657089427692,"user_tz":-540,"elapsed":12061,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"9054a9fe-fc2e-4390-e65d-2bbfdee24ccc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f581c7a6b74bd18903604993fec894"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fce563cfe9c241c4b95675d2f935b0c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"653cd24ff9db41b5be4e408788ad7868"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["['안', '##녕', '##하', '##세', '##요', '.', '반', '##갑', '##습', '##니다', '.']\n","{'input_ids': [101, 9521, 118741, 35506, 24982, 48549, 119, 9321, 118610, 119081, 48345, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}]},{"cell_type":"code","source":["# 토크나이징\n","train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n","val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n","test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")"],"metadata":{"id":"JIL0teQXNTDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bert-base-multilingual-cased 사전학습모형으로부터 분류기 모형을 생성\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":161,"referenced_widgets":["e733d90da32d4a0fb7fa2b24bb8aaa6b","6f0a89aa20a9423cb785a2b7db0b47b6","91316c8f5bb345b0b51bd507c48e2bfb","da3daac4b75f4a379eaccfaed83a0b36","dee4d6002fc848e783470318351b8cdf","3b94577fba0344908827fbca9146406e","5856fb66a69a498c984a34e0f1ad02c0","3ad47530c879443daeb0d3d46b4093b8","6caedb01a9f34e8ba9e3469fa943f9a7","cad4b0ad9743406a8d80c2bd1b1e23df","074744d89f204c9b920c8c690edbdb68"]},"id":"zlAuBfiHNevj","executionInfo":{"status":"ok","timestamp":1657089470710,"user_tz":-540,"elapsed":30353,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"91b2d596-1174-4ddd-e14c-e19037d16b16"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/681M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e733d90da32d4a0fb7fa2b24bb8aaa6b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["**데이터셋 변환**"],"metadata":{"id":"7BM1wgv2N7w3"}},{"cell_type":"code","source":["import torch\n","\n","class OurDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"metadata":{"id":"PU_ZYboSNGXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = OurDataset(train_input, y_train)\n","val_dataset = OurDataset(val_input, y_val)\n","test_dataset = OurDataset(test_input, y_test)"],"metadata":{"id":"YYb1-JJJNWDK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**정확도 함수 정의**"],"metadata":{"id":"3mCmeP_8N-HP"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["1d77456f60974e1bbbe54953bb4005cb","8df437f2793944ae911362ef90f93a88","a9a6ab36d2824c12a003826e96a7ce81","ddaea7dcd8aa4a0bac2442920a3dfb98","dd6d2640f040440191e14a2bbca1b82c","7747640cf0344e1eb074716ab8e55710","f252cfa89bb24ca0a9677d4c58ba50c3","bc6226d7b73147c19d47696f64c256c3","ce4dc34045be4e03a92397ebec80d207","c16cf030afe84236bb2fb90de9768bce","ce9bef725e134692a28d29b2d6e512cc"]},"id":"KTqW_TOONO4P","executionInfo":{"status":"ok","timestamp":1657089471838,"user_tz":-540,"elapsed":1132,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"b4c90ab3-23e3-4ca4-c37d-ba18bf3bf8ac"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d77456f60974e1bbbe54953bb4005cb"}},"metadata":{}}]},{"cell_type":"markdown","source":["**미세조정 학습**"],"metadata":{"id":"fh_PRsxwOAvn"}},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","# Trainer에서 사용할 하이퍼파라미터 지정\n","training_args = TrainingArguments(\n","    output_dir='./results',          # 모형 예측이나 체크포인트 출력 폴더, 반드시 필요함\n","    num_train_epochs=3,              # 학습 에포크 수\n","    evaluation_strategy=\"steps\",      # eval_steps 마다 검증 데이터셋에 대한 평가 지표를 출력\n","    eval_steps = 500,\n","    per_device_train_batch_size=8,   # 학습에 사용할 배치 사이즈\n","    per_device_eval_batch_size=16,   # 평가에 사용할 배치 사이즈\n","    warmup_steps=200,                # 학습률 스케줄러의 warmup 구간 설정\n","    weight_decay=0.01,               # AdamW의 가중치 감쇠도\n",")\n","\n","# Trainer 객체 생성\n","trainer = Trainer(\n","    model=model,                     # 학습할 모형\n","    args=training_args,              # 위에서 정의한 학습 매개변수\n","    train_dataset=train_dataset,     # 훈련 데이터셋\n","    eval_dataset=val_dataset,        # 검증 데이터셋\n","    compute_metrics=compute_metrics,\n",")\n","\n","# 미세조정학습 실행\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1JU6XsmVNoLX","executionInfo":{"status":"ok","timestamp":1657091428289,"user_tz":-540,"elapsed":1956459,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"5963cc92-656c-4acb-e2a2-d22bb92e1dad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 8282\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3108\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3108' max='3108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3108/3108 32:18, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.567500</td>\n","      <td>0.583939</td>\n","      <td>0.729446</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.570500</td>\n","      <td>0.543302</td>\n","      <td>0.769286</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.566400</td>\n","      <td>0.540187</td>\n","      <td>0.769286</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.560300</td>\n","      <td>0.540476</td>\n","      <td>0.769286</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.559800</td>\n","      <td>0.540115</td>\n","      <td>0.770373</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.529100</td>\n","      <td>0.495530</td>\n","      <td>0.775081</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 2761\n","  Batch size = 16\n","Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n","***** Running Evaluation *****\n","  Num examples = 2761\n","  Batch size = 16\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n","***** Running Evaluation *****\n","  Num examples = 2761\n","  Batch size = 16\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n","***** Running Evaluation *****\n","  Num examples = 2761\n","  Batch size = 16\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n","***** Running Evaluation *****\n","  Num examples = 2761\n","  Batch size = 16\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n","***** Running Evaluation *****\n","  Num examples = 2761\n","  Batch size = 16\n","Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=3108, training_loss=0.556299690727715, metrics={'train_runtime': 1939.146, 'train_samples_per_second': 12.813, 'train_steps_per_second': 1.603, 'total_flos': 4034713478410080.0, 'train_loss': 0.556299690727715, 'epoch': 3.0})"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["**모델 평가**"],"metadata":{"id":"DMpb7cO5OE8l"}},{"cell_type":"code","source":["trainer.evaluate(eval_dataset=test_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259},"id":"JXaU_Nd6NxB6","executionInfo":{"status":"ok","timestamp":1657091504404,"user_tz":-540,"elapsed":76120,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"48f7d60d-25f1-4d4e-8256-7e4228dcf78b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 3682\n","  Batch size = 16\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  if __name__ == '__main__':\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='231' max='231' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [231/231 01:15]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'epoch': 3.0,\n"," 'eval_accuracy': 0.7705051602390005,\n"," 'eval_loss': 0.49175286293029785,\n"," 'eval_runtime': 76.1335,\n"," 'eval_samples_per_second': 48.362,\n"," 'eval_steps_per_second': 3.034}"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["# KcBERT에 대한 파이토치 기반 미세조정학습"],"metadata":{"id":"7JEHn4BGSO_1"}},{"cell_type":"code","source":["# GPU 메모리 확보 (multilingual BERT을 실행했다면 필수)\n","\n","del model\n","del trainer\n","torch.cuda.empty_cache()"],"metadata":{"id":"VpFG8wIqR9wZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**토크나이저 및 모델(분류기) 설정**"],"metadata":{"id":"zovWRK7oS94x"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","from transformers import BertModel\n","\n","tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')\n","\n","# 토크나이징 예시\n","print(tokenizer.tokenize(\"안녕하세요. 반갑습니다.\"))\n","inputs = tokenizer(\"안녕하세요. 반갑습니다.\")\n","print(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UlyAfVI-S-xP","executionInfo":{"status":"ok","timestamp":1657164896377,"user_tz":-540,"elapsed":3897,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"504390b4-3a54-4667-a4f4-8afcd947ab87"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['안녕', '##하세요', '.', '반', '##갑', '##습니다', '.']\n","{'input_ids': [2, 19017, 8482, 17, 1483, 4981, 8046, 17, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"]}]},{"cell_type":"code","source":["# 토크나이징\n","train_input = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n","val_input = tokenizer(X_val, truncation=True, padding=True, return_tensors=\"pt\")\n","test_input = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")"],"metadata":{"id":"EPkcpxtLTPUA","executionInfo":{"status":"ok","timestamp":1657164901469,"user_tz":-540,"elapsed":5101,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# KcBERT 사전학습모형 로드\n","bert_model = BertModel.from_pretrained('beomi/kcbert-base')\n","bert_model.config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEYC-E9rTZs1","executionInfo":{"status":"ok","timestamp":1657164903600,"user_tz":-540,"elapsed":2140,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"fff8c0c5-70d9-4954-9619-fb6db6331ab4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["BertConfig {\n","  \"_name_or_path\": \"beomi/kcbert-base\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 300,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30000\n","}"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["**데이터셋 및 데이터로더 변환**"],"metadata":{"id":"A_M6giMXT26V"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","\n","class OurDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"metadata":{"id":"8azsUpRwTrb1","executionInfo":{"status":"ok","timestamp":1657164903601,"user_tz":-540,"elapsed":5,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Dataset\n","train_dataset = OurDataset(train_input, y_train)\n","val_dataset = OurDataset(val_input, y_val)\n","test_dataset = OurDataset(test_input, y_test)"],"metadata":{"id":"f2Lzr6zkTzk6","executionInfo":{"status":"ok","timestamp":1657164903601,"user_tz":-540,"elapsed":4,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# DataLoader\n","train_loader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n","val_loader = DataLoader(val_dataset, batch_size=16)\n","test_loader = DataLoader(test_dataset, batch_size=16)"],"metadata":{"id":"kX9DJ2_kT06q","executionInfo":{"status":"ok","timestamp":1657164903602,"user_tz":-540,"elapsed":5,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["**모델 정의**"],"metadata":{"id":"PLG-4ihHUIJ7"}},{"cell_type":"code","source":["# BERT를 포함한 신경망 모형\n","class MyModel(torch.nn.Module):\n","    def __init__(self, pretrained_model, token_size, num_labels): \n","        super(MyModel, self).__init__()\n","        self.token_size = token_size\n","        self.num_labels = num_labels\n","        self.pretrained_model = pretrained_model\n","\n","        # 분류기 정의\n","        self.classifier = torch.nn.Linear(self.token_size, self.num_labels)\n","\n","    def forward(self, inputs):\n","        # BERT 모형에 입력을 넣고 출력을 받음\n","        outputs = self.pretrained_model(**inputs)\n","        # BERT 출력에서 CLS 토큰에 해당하는 부분만 가져옴\n","        bert_clf_token = outputs.last_hidden_state[:,0,:]\n","        \n","        return self.classifier(bert_clf_token)\n","\n","# token_size는 BERT 토큰과 동일\n","model = MyModel(bert_model, num_labels=2, token_size=bert_model.config.hidden_size)"],"metadata":{"id":"LmNhgkqfUCkT","executionInfo":{"status":"ok","timestamp":1657164903602,"user_tz":-540,"elapsed":4,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["**모델 학습**"],"metadata":{"id":"K0KbpPzfUgrU"}},{"cell_type":"code","source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","import torch.nn.functional as F\n","import time\n","\n","# GPU 가속을 사용할 수 있으면 device를 cuda로 설정하고, 아니면 cpu로 설정\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","model.to(device)  # 모형을 GPU로 복사\n","model.train()     # 학습모드로 전환\n","\n","# 옵티마이저를 트랜스포머가 제공하는 AdamW로 설정\n","optim = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) # 가중치 감쇠 설정\n","criterion = torch.nn.CrossEntropyLoss()    # 멀티클래스이므로 크로스 엔트로피를 손실함수로 사용\n","\n","num_epochs = 3      # 학습 epoch를 3회로 설정\n","total_training_steps = num_epochs * len(train_loader)\n","# 학습 스케줄러 설정\n","scheduler = get_linear_schedule_with_warmup(optimizer=optim,\n","                                            num_training_steps=total_training_steps,\n","                                            num_warmup_steps=200)\n","\n","start = time.time() # 시작시간 기록\n","eval_steps = 500\n","step = 0\n","\n","model.train()     # 학습모드로 전환"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcAhoU34VIzC","executionInfo":{"status":"ok","timestamp":1657164911328,"user_tz":-540,"elapsed":7730,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"9bff20eb-43f5-495c-d893-538ddffc4cdd"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["MyModel(\n","  (pretrained_model): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(300, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    train_loss = 0\n","    for batch in train_loader:\n","        optim.zero_grad()     # 그래디언트 초기화\n","\n","        # 배치에서 label을 제외한 입력만 추출하여  GPU로 복사\n","        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'} \n","        labels = batch['labels'].to(device) # 배치에서 라벨을 추출하여 GPU로 복사\n","        outputs = model(inputs) # 모형으로 결과 예측\n","\n","        # 두 클래스에 대해 예측하고 각각 비교해야 하므로 labels에 대해 원핫인코딩을 적용한 후에 손실을 게산\n","        loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss 계산\n","        train_loss += loss\n","\n","        loss.backward() # 그래디언트 계산\n","        optim.step()    # 가중치 업데이트\n","        scheduler.step() # 스케줄러 업데이트\n","        \n","        step += 1\n","        if step % eval_steps == 0:  # eval_steps 마다 경과한 시간과 loss를 출력\n","            with torch.no_grad():   # 학습 X (그래디언트 계산 X)\n","                val_loss = 0\n","                model.eval()        # 평가모드로 전환\n","\n","                for batch in val_loader:\n","                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n","                    labels = batch['labels'].to(device)\n","                    outputs = model(inputs)\n","\n","                    loss = criterion(outputs, F.one_hot(labels, num_classes=2).float()) # loss 계산\n","                    val_loss += loss\n","\n","                avg_val_loss = val_loss / len(val_loader)\n","\n","            avg_train_loss = train_loss / eval_steps    # eval_steps의 평균 loss 계산\n","            \n","            elapsed = time.time() - start\n","            print('Step %d, elapsed time: %.2f, train loss: %.4f, validation loss: %.4f' \n","                  % (step, elapsed, avg_train_loss, avg_val_loss))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMIo5xydV0XE","executionInfo":{"status":"ok","timestamp":1657166246431,"user_tz":-540,"elapsed":1335144,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"7650bb67-4851-423b-87d8-8b97fe8f04d1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # Remove the CWD from sys.path while we load stuff.\n"]},{"output_type":"stream","name":"stdout","text":["Step 500, elapsed time: 215.10, train loss: 0.4338, validation loss: 0.3216\n","Step 1000, elapsed time: 431.69, train loss: 0.7569, validation loss: 0.2759\n","Step 1500, elapsed time: 647.87, train loss: 0.1343, validation loss: 0.3381\n","Step 2000, elapsed time: 864.52, train loss: 0.2860, validation loss: 0.3151\n","Step 2500, elapsed time: 1080.73, train loss: 0.0323, validation loss: 0.3729\n","Step 3000, elapsed time: 1297.50, train loss: 0.0570, validation loss: 0.3868\n"]}]},{"cell_type":"markdown","source":["**모델 평가(테스트)**"],"metadata":{"id":"NKc-YOUJXZ7R"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","metric= load_metric(\"accuracy\")\n","model.eval()\n","\n","for batch in test_loader:\n","    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n","    labels = batch['labels'].to(device)\n","    \n","    with torch.no_grad(): # 학습 X (그래디언트 계산 X)\n","        outputs = model(inputs)\n","\n","    predictions = torch.argmax(outputs, dim=-1)\n","\n","metric.compute(predictions=predictions, references=labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WynxO8DzWSqn","executionInfo":{"status":"ok","timestamp":1657166302810,"user_tz":-540,"elapsed":56388,"user":{"displayName":"CaFe CoKe","userId":"11886396836022920274"}},"outputId":"64436619-8a86-47ef-e42c-8e6b8ea18b18"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  # Remove the CWD from sys.path while we load stuff.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 1.0}"]},"metadata":{},"execution_count":12}]}]}
# Text Mining
[파이썬 텍스트 마이닝 완벽 가이드](https://wikibook.co.kr/textmining/) 를 기반으로 공부합니다.

 - 기본 베이스 코드 : https://github.com/wikibook/textmining
<br><br>
 - 사용하는 기본 라이브러리
   - [KoNLPy](https://konlpy.org/ko/latest/)
   - [NLTK](https://www.nltk.org/api/nltk.html)
   - [scikit-learn](https://scikit-learn.org/stable/modules/classes.html)
   - [Tensorflow](https://www.tensorflow.org/api_docs/python/tf?hl=ko)
   - [Keras](https://keras.io/api/)
   - [PyTorch](https://pytorch.org/docs/stable/index.html)
   - [Transformers](https://huggingface.co/docs/transformers/index)
 <br><br>
 - Chapter 설명
   - Chapter 2 : 텍스트 전처리 (Text Preprocessing)
   - Chapter 3 : 그래프 & 워드 클라우드 (Graph & WordCloud)
   - Chapter 4 : 카운트 기반의 문서 표현 (Count based Document Representation)
   - Chapter 5 : BoW 기반의 문서 분류 (Bag of Word based Document Classification)
   - Chapter 6 : 차원 축소 (Dimensionality Reduction)
   - Chapter 7 : 토픽 모델링 (Topic Modeling)
   - Chapter 8 : 감성 분석 (Sentiment Analysis)
   - Chapter 10 : 딥러닝을 이용한 문서 분류 (Document Classification using Deep Learning)
   - Chapter 11 : Word2Vec, ELMo, Doc2Vec의 이해 (Understanding Word2Vec, ELMo, Doc2Vec)
   - Chapter 12 : CNN을 응용한 문서 분류 (Document Classification using CNN)
   - Chapter 13 : [어텐션과 트랜스포머 (Attention & Transfomers)](https://github.com/CaFeCoKe/NLP_BERT_GPT/tree/main/Chapter%203)
   <br> - 다른 rapository에 설명(현 Chapter부터 설명은 여기에 추가)
   - Chapter 14 : BERT (Bidirectional Encoder Representations from Transformers) 의 이해와 간단한 활용
   - Chapter 15 : BERT 사전학습(pre-trained) 모형에 대한 미세조정(Fine-tuning) 학습
   - Chapter 16 : 한국어 문서에 대한 BERT 황용
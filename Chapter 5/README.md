# Chapter 5. BoW 기반의 문서 분류

- 머신러닝(Machine Learning)에서의 문서 분류(Document Classification) 과정<br><br>
![Document_classfication](https://user-images.githubusercontent.com/86700191/169681358-4821a9d8-b325-491f-bc99-42ef1329fbe7.png)
<br><br>
  - 데이터 정제 및 전처리 : 텍스트 전처리 과정을 통해 대상 말뭉치를 특성 데이터로 변환한다.
  - 데이터 분리 : 대상 데이터를 학습 데이터와 평가 데이터로 분리한다. 
  - 머신러닝 학습 : 학습 데이터에 대해 머신러닝 알고리즘을 적용하여 학습된 예측모형을 얻는다.
  - 평가 : 학습된 예측모형에 대해 평가 데이터를 이용하여 평가를 실시한다. 
  - 최종모형 도출 : 평가를 통해 최적의 최종 모형을 도출한다.
  - 예측 : 실제 문제에 대해 예측모형을 적용하기 위해, 실제 텍스트를 전처리 과정을 통해 특성 데이터를 추출하고 최종 모형으로 문서를 분류한다.
<br><br>
- 나이브 베이즈 분류기 (Naive Bayse Classifier) : 사건 B가 주어졌을 때 사건 A가 일어날 확률인 P(A|B), 조건부 확률과 베이즈 정리를 이용한 분류기<br><br>
  - 나이브 : 예측한 특징이 상호 독립적이라는 가정 하에 확률 계산을 단순화, 모든 변수(특징)들이 동등하다는 것을 의미한다.<br><br>
  - 베이즈 : 입력 특징이 클래스 전체의 확률 분포 대비 특정 클래스에 속할 확률을 베이즈 정리를 기반으로 계산한다.<br><br>
  - 장점 : 간단하고 빠르며, 효율적이다. 잡음과 누락 데이터를 잘 처리한다. 훈련을 할 때 데이터의 크기에 상관 없이 잘 동작한다. 예측을 위한 추정 확률을 쉽게 얻을 수 있다.<br><br>
  - 단점 : 모든 특징이 동등하게 중요하고, 독립이라는 가정이 잘못된 경우가 자주 있다. 수치 특징이 많은 데이터셋에는 이상적이지 않다.<br><br>
  - 베이즈 정리 (Bayesian Rule) : 사건 B가 발생함으로써(P(B)=1) 사건 A의 확률이 어떻게 변화하는지를 표현한 정리<br><br>
  ![bayesian rule](https://user-images.githubusercontent.com/86700191/169699073-1481b857-86f3-49f6-ae46-df917ece5acf.PNG)
  <br><br>
  P(A|B): 사후확률(posterior). 사건 B가 발생한 후 갱신된 사건 A의 확률<br>
  P(A): 사전확률(prior). 사건 B가 발생하기 전에 가지고 있던 사건 A의 확률<br>
  P(B|A): 가능도(likelihood). 사건 A가 발생한 경우 사건 B의 확률<br>
  P(B): 정규화 상수(normalizing constant) 또는 증거(evidence). 확률의 크기 조정
<br><br>
- 로지스틱 회귀분석 (Logistic Regression) : 회귀를 사용하여 데이터가 어떤 범주에 속할 확률을 0에서 1 사이의 값으로 예측하고 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류해주는 지도 학습 알고리즘<br><br>
  - 로지스틱 회귀의 종류
    - 이진 로지스틱 회귀 : 범주형 응답에 대해 가능한 결과는 두 가지이다. 0 아니면 1이다. 예를 들어 시험의 결과로 합격과 불합격으로 나누어지는 경우를 들 수 있다.<br><br>
    - 다항 로지스틱 회귀 : 응답 변수에 순서가 없는 3개 이상의 변수가 포함될 수 있다. 예를 들어 대학교 지원 결과로 합격, 불합격, 예비로 나누어지는 경우를 둘 수 있다.<br><br>
    - 순서 로지스틱 회귀 : 다항 회귀와 마찬가지로 3개 이상의 변수가 있을 수 있다. 그러나 측정에는 순서가 있다. 예를 들어 1에서 5까지의 척도로 호텔을 평가하는 경우를 들 수 있다.<br><br>
  - 로지스틱 회귀의 가정
    - 이진 로지스틱 회귀에서는 응답 변수가 이진이어야 한다.
    - 원하는 결과는 응답 변수의 요인 수준 1로 표시되어야 하며 원하지 않는 결과는 0이다.
    - 의미를 가지는 변수만 포함해야 한다.
    - 독립변수는 본질적으로 서로 독립적이어야 합니다. 다중 공선성이 거의 또는 전혀 없어야 한다.
    - 로그 오즈와 독립 변수는 선형적으로 관련되여야한다.
    - 로지스틱 회귀는 대규모 크기 샘플에만 적용해야 한다.
<br><br> 
  - 로지스틱 회귀의 단계
    1. 모든 속성(feature)들의 계수(coefficient)와 절편(intercept)을 0으로 초기화한다.
    2. 각 속성들의 값(value)에 계수(coefficient)를 곱해서 log-odds를 구한다.
    3. log-odds를 sigmoid 함수에 넣어서 [0,1] 범위의 확률을 구한다.
       - log-odds : 사건이 발생할 확률을 발생하지 하지 않을 확률로 나눈 값이 odds다. 이 값에 log를 취한것.<br><br>
       ![odds](https://user-images.githubusercontent.com/86700191/170240378-ef7f28f7-08a5-47cd-bc6e-1a95721a64d8.PNG)
<br><br>
- 릿지 회귀분석 (Ridge Regression) : 최소제곱법과 매우 유사하나, 가중치들의 제곱합(squared sum of weights)을 최소화하는 것을 추가적인 제약 조건으로 한다.
L2 규제를 적용하여 선형회귀분석의 과대적합 문제를 해소하려는데에 목적이 있다.<br><br>
![Ridge](https://user-images.githubusercontent.com/86700191/170689524-0284225b-da83-41a8-9041-6995bf158fa8.PNG)
<br><br>
n은 데이터의 개수<br>
p는 feature의 개수. 단순선형회귀 p=1이 되고, 다중회귀에서는 p>=2<br>
y는 실제 target의 값<br>
ß는 가중치로서 OLS의 feature 계수라고 볼 수 있다.<br>
λ는 ridge 모형의 하이퍼파라미터로 alpha값을 뜻함.<br>
첫째 항의 의미는 편차의 제곱합이다. 이는 OLS에서 feature 계수 ß를 도출하는 식이다.<br>
둘째 항의 의미하는 것이 L2 규제이다..<br><br>
- 라쏘 회귀분석 : L1규제를 통해 feature간 다중공선성을 제거할 수 있는 방법이며, 추정계수의 절댓값 합을 최소로 하는것이라 볼 수 있다.<br><br>
![Lasso](https://user-images.githubusercontent.com/86700191/170691719-5502aa12-7673-4971-a667-630ed39a4863.PNG)
<br><br>
n은 데이터의 개수<br>
p는 feature의 개수. 단순선형회귀 p=1이 되고, 다중회귀에서는 p>=2<br>
y는 실제 target의 값<br>
ß는 가중치로서 OLS의 feature 계수라고 볼 수 있다.<br>
λ는 라쏘회귀의 하이퍼파라미터로 규제 강도를 의미한다.<br>
  - 만약 λ = 0 이라면 OLS와 동일한 형태의 결과가 출력된다.
  - 만약 λ가 무한의 값이라면 아무 feature도 선택되지 않는다.
  - λ가 증가하면 편향성(bias)가 증가하지만 variance는 감소한다.
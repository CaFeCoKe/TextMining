# Chapter 7. 토픽 모델링

- 토픽 모델링 (Topic Modeling) : 다양한 문서 집합에 내재한 토픽, 즉 주제를 파악할 때 쓰는 방법이다. 예측 보다는 내용의 분석을 목적으로 하는 기법으로 새로운 문서에 대한 문서에 대한 예측에는 잘 쓰이지 않는다.<br><br>

- LDA (Latent Dirichlet Allocation) : 잠재 디리클레 할당이라고 불리는 LDA는 문서들은 토픽들의 혼합으로 구성되어 있으며, 토픽들은 확률 분포에 기반하여 단어들을 생성한다고 가정한다. 데이터가 주어지면, LDA는 문서가 생성되던 과정을 역추적한다.<br><br>
  - 모델 구조<br><br>
  ![LDA_model](https://user-images.githubusercontent.com/86700191/172824271-87cdfbbd-19d4-4c96-a6fe-e3a8f2090bd5.png)
  <br><br>
  D는 말뭉치 전체 문서 개수<br>
  N은 d번째 문서의 단어 수<br>
  K는 전체 토픽 수 (하이퍼 파라미터)<br>
  α는 디리클레 파라미터 (하이퍼 파라미터)<br>
  β는 토픽 파라미터 (하이퍼 파라미터)<br>
  ϕk 는 k번째 토픽에 해당하는 벡터 <br>
  θd는 d번째 문서가 가진 토픽 비중을 나타내는 벡터 <br>
  zd,n은 d번째 문서 n번째 단어가 어떤 토픽에 해당하는지 할당해주는 역할 <br>
  wd,n 은 문서에 등장하는 단어를 할당해주는 역할<br><br>
  - LDA의 가정
    1. 문서에 사용할 단어의 개수 N을 정한다.
    2. 문서에 사용할 토픽의 혼합을 확률 분포에 기반하여 결정한다. 
    3. 문서에 사용할 각 단어를 (아래와 같이) 정한다. (반복)
       1. 토픽 분포에서 토픽 T를 확률적으로 고른다.
       2. 선택한 토픽 T에서 단어의 출현 확률 분포에 기반해 문서에 사용할 단어를 고른다.<br>
    
    위와 같은 과정으로 문서가 작성되었다는 가정 하에 LDA는 토픽을 뽑아내기 위하여 위 과정을 역으로 추적하는 역공학(reverse engneering)을 수행한다.<br><br>
  - LDA 수행 과정
    1. 사용자는 알고리즘에게 토픽의 개수 k를 알려준다.
    2. 모든 단어를 k개 중 하나의 토픽에 할당한다.
    3. 이제 모든 문서의 모든 단어에 대해서 아래의 사항을 반복 진행한다. (iterative)
       1. 어떤 문서의 각 단어 w는 자신은 잘못된 토픽에 할당되어져 있지만, 다른 단어들은 전부 올바른 토픽에 할당되어져 있는 상태라고 가정한다. 이에 따라 단어 w는 아래의 두 가지 기준에 따라서 토픽이 재할당된다..
          - p(topic t | document d) : 문서 d의 단어들 중 토픽 t에 해당하는 단어들의 비율
          - p(word w | topic t) : 각 토픽들 t에서 해당 단어 w의 분포<br><br>
  - LDA 모형 평가
    - 혼잡도 (Perplexity) : 추정한 디리클레 모형이 주어진 문서 집합을 얼마나 유사하게 생성할 수 있는지 나타낸다. 값이 작을수록 좋다.
    - 토픽 응집도 (Coherence) : 각 토픽에서 상위 비중을 차지하는 단어들이 의미적으로 유사한지 나타낸다. 값이 클수록 좋다.
  - LSA vs LDA 
    - LSA : DTM을 차원 축소 하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.
    - LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다.